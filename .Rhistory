n <- 100
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
Y.CP <- ctingP_uni_rcpp(as.vector(Y), as.vector(Y))
w <- rexp(n)
X.sort <- as.matrix(X[order(Y), ])
Y.sort <- Y[order(Y)]
rank.y.in.Y <- rankAinB_rcpp(Y.sort, Y.sort)
test1 <- CVMNW_K2B_R(X = X, Y = Y.CP, h = 1.5, p_Y = rep(1 / n, n))
test2 <- CVMNW_K2B_rcpp(X = X, Y = Y.CP, h = 1.5, p_Y = rep(1 / n, n))
test3 <- CVDNWuniY_K2B_rcpp(X = X.sort, Y = Y.sort, h = 1.5,
rank_y_in_Y = rank.y.in.Y, p_y = rep(1 / n, n))
test4 <- CVDNWuniY_K2B_rcpp_n1(X = X, Y = Y, h = 1.5, y = Y, p_y = rep(1 / n, n))
test5 <- CVDNWuniY_K2B_rcpp_n1(X = X.sort, Y = Y.sort, h = 1.5, y = Y.sort, p_y = rep(1 / n, n))
sum(abs(test1 - test2))
sum(abs(test1 - test3))
sum(abs(test1 - test4))
sum(abs(test1 - test5))
ggplot2::autoplot(
microbenchmark::microbenchmark(
R = CVMNW_K2B_R(X = X, Y = Y.CP, h = 1.5, p_Y = rep(1 / n, n)),
Rcpp = CVMNW_K2B_rcpp(X = X, Y = Y.CP, h = 1.5, p_Y = rep(1 / n, n)),
Rcpp_uniY = CVDNWuniY_K2B_rcpp(X = X.sort, Y = Y.sort, h = 1.5,
rank_y_in_Y = rank.y.in.Y, p_y = rep(1 / n, n)),
Rcpp_uniY_n1 = CVDNWuniY_K2B_rcpp_n1(X = X, Y = Y, h = 1.5, y = Y, p_y = rep(1 / n, n))
)
)
library(MYHRcpp)
rep(c(1, 2), 10)
rep(c(1, 2), 11)
?c
?nlminb
quantile(1:10, probs = c(0, 1))
quantile(1:10, probs = c(0, 0.1, 1))
a <- matrix(1:5, 5, 1)
a
a[c(1, 4, 2)]
order(a)
sort(unique(a))
library(MYHRcpp)
?nmk
n <- 1000
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
x <- as.matrix(seq(-3, 3, 0.1))
LOOCV(X = X, Y = Y)
LOOCV(X = X, Y = Y, regression = "mean")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "K4_Biweight")
n <- 500
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
x <- as.matrix(seq(-3, 3, 0.1))
### selecting bandwidth using leave-one-out cross-validation
LOOCV(X = X, Y = Y)
LOOCV(X = X, Y = Y, regression = "mean")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "K4_Biweight")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "Gaussian")
LOOCV(X = X, Y = Y, regression = "distribution")
library(MYHRcpp)
n <- 500
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
x <- as.matrix(seq(-3, 3, 0.1))
### selecting bandwidth using leave-one-out cross-validation
LOOCV(X = X, Y = Y)
LOOCV(X = X, Y = Y, regression = "mean")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "K4_Biweight")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "Gaussian")
LOOCV(X = X, Y = Y, regression = "distribution")
LOOCV(X = X, Y = Y, regression = "mean", method = "nlminb")
LOOCV(X = X, Y = Y, regression = "mean", method = "nmk")
library(dfoptim)
LOOCV(X = X, Y = Y, regression = "mean", method = "nmk")
library(MYHRcpp)
n <- 500
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
x <- as.matrix(seq(-3, 3, 0.1))
### selecting bandwidth using leave-one-out cross-validation
LOOCV(X = X, Y = Y)
LOOCV(X = X, Y = Y, regression = "mean")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "K4_Biweight")
LOOCV(X = X, Y = Y, regression = "mean", kernel = "Gaussian")
LOOCV(X = X, Y = Y, regression = "mean", method = "nlminb")
LOOCV(X = X, Y = Y, regression = "distribution")
LOOCV(X = X, Y = Y, regression = "distribution", kernel = "K4_Biweight")
LOOCV(X = X, Y = Y, regression = "distribution", kernel = "Gaussian")
LOOCV(X = X, Y = Y, regression = "distribution", method = "nlminb")
LOOCV(X = X, Y = Y, regression = "distribution", dist.mode = "sample")
LOOCV(X = X, Y = Y, regression = "distribution", dist.mode = "quantile")
LOOCV(X = X, Y = Y, regression = "distribution", dist.mode = "sample",
dist.sample.control = list(SN = 10, seed = 123))
LOOCV(X = X, Y = Y, regression = "distribution", dist.mode = "quantile",
dist.quantile.control = list(QN = 10))
library(MYHRcpp)
library(MYHRcpp)
yhat0 <- NW(X = X, Y = Y)
dim(X)
dim(Y)
yhat0 <- NW(X = X, Y = Y)
yhat1 <- NW(X = X, Y = Y, x = x)
yhat2 <- NW(X = X, Y = Y, x = x, regression = "mean")
yhat3 <- NW(X = X, Y = Y, x = x, regression = "mean",
kernel = "K4_Biweight")
yhat4 <- NW(X = X, Y = Y, x = x, regression = "mean",
kernel = "K4_Biweight", bandwidth = 0.5)
yhat5 <- NW(X = X, Y = Y, x = x, regression = "distribution")
yhat6 <- NW(X = X, Y = Y, x = x, regression = "distribution",
y = sort(unique(Y)))
plot(X, Y, cex = 0.5)
lines(x, yhat)
plot(X, Y, cex = 0.5)
lines(x, yhat0)
yhat0
plot(X, Y, cex = 0.5)
lines(x, NW(X = X, Y = Y, x = x))
lines(x, NW(X = X, Y = Y, x = x, bandwidth = 0.1), col = 2)
lines(x, NW(X = X, Y = Y, x = x, bandwidth = 1), col = 3)
lines(x, NW(X = X, Y = Y, x = x, bandwidth = 2), col = 4)
RW <- colSums(outer(sample(1:n, size = n, replace = TRUE), 1:n, FUN = "=="))
hhat.boot <- LOOCV(X = X, Y = Y, wi.boot = RW)
n <- 100
p <- 5
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
LOOCV(X = X, Y = Y)
yhat <- NW(X = X, Y = Y, x = X)
plot(Y, yhat, cex = 0.5)
lines(c(-3, 3), c(-3, 3))
library(MYHRcpp)
library(MYHRcpp)
floor(100/6)
floor(100/6) * 6
sample(1:5, 15)
sample(1:5, 5)
library(MYHRcpp)
library(MYHRcpp)
##### univariate covariate
n <- 500
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
x <- as.matrix(seq(-3, 3, 0.1))
KfoldCV(X = X, Y = Y)
LOOCV(X = X, Y = Y)
KfoldCV(X = X, Y = Y, K = 2)
KfoldCV(X = X, Y = Y, K = 10)
KfoldCV(X = X, Y = Y, K = 2, regression = "distribution")
library(MYHRcpp)
##### univariate covariate
n <- 500
p <- 1
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
x <- as.matrix(seq(-3, 3, 0.1))
### selecting bandwidth using leave-one-out cross-validation
LOOCV(X = X, Y = Y)
LOOCV(X = X, Y = Y, regression = "distribution")
KfoldCV(X = X, Y = Y, regression = "distribution")
KfoldCV(X = X, Y = Y, regression = "distribution", K = 2)
library(MYHRcpp)
library(MYHRcpp)
n <- 500
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
SDR1 <- cumuSIR(X = X, Y = Y)
SDR1
A <- matrix(rnorm(5*5), 5, 5)
A
chol(A)
chol(t(A) %*% A)
?chol(A)
B <- chol(t(A) %*% A)
B %*% B
A %*% A
library(MYHRcpp)
A <- matrix(rnorm(50 ^ 2), 50, 50)
B <- t(A) %*% A
test1 <- chol(B)
test2 <- chol_rcpp(B)
sum(abs(test1 - test2))
sum(abs(t(test1) %*% test1 - B))
ggplot2::autoplot(
microbenchmark::microbenchmark(
chol = chol(B),
Rcpp = chol_rcpp(B)
)
)
n <- 500
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
eps = 1e-7
X <- as.matrix(X)
Y <- as.matrix(Y)
number_n <- dim(X)[1]
number_p <- dim(X)[2]
# calculating induced response process 1(Y_i\leq y) at y=Y_i
if (dim(Y)[2] == 1)
{
Y.CP <- ctingP_uni_rcpp(as.vector(Y), as.vector(Y))
}else
{
Y.CP <- ctingP_rcpp(Y, Y)
}
# centralizing and standardizing covariates
eigen_invVarX <- eigen(inv_sympd_rcpp(var(X)))
eigen_invVarX
normalizing <- eigen_invVarX$vectors %*% diag(eigen_invVarX$values) ^ 0.5 %*% t(eigen_invVarX$vectors)
normalizing
normalizing %*% normalizing %*% var(X)
# centralizing and standardizing covariates
eigen_invVarX <- eigen(inv_sympd_rcpp(var(X) + eps * diag(number_p)))
normalizing <- eigen_invVarX$vectors %*% diag(eigen_invVarX$values) ^ 0.5 %*% t(eigen_invVarX$vectors)
normalizing %*% normalizing %*% var(X)
eigen_invVarX <- eigen(inv_sympd_rcpp(var(X) + eps * diag(number_p)))
normalizing <- eigen_invVarX$vectors %*%
diag(eigen_invVarX$values) ^ 0.5 %*%
t(eigen_invVarX$vectors)
X.cs <- t(normalizing %*% (t(X) - colMeans(X)))
var(X.cs)
var(X.cs)-diag(p)
sum(abs(var(X.cs)-diag(p)))
# calculating m(y)=\E[X_i 1(Y_i\leq y)]
m.y <- t(X.cs) %*% Y.CP / number_n
# calculating K=\E[m(Y_i)m(Y_i)^T]
Km <- m.y %*% t(m.y) / number_n
RR <- eigen_rcpp(Km)
Bhat <- normalizing %*% RR$vector
X <- as.matrix(X)
Y <- as.matrix(Y)
number_n <- dim(X)[1]
number_p <- dim(X)[2]
# calculating induced response process 1(Y_i\leq y) at y=Y_i
if (dim(Y)[2] == 1)
{
Y.CP <- ctingP_uni_rcpp(as.vector(Y), as.vector(Y))
}else
{
Y.CP <- ctingP_rcpp(Y, Y)
}
eigen_invVarX <- eigen(inv_sympd_rcpp(var(X) + eps * diag(number_p)))
normalizing <- eigen_invVarX$vectors %*%
diag(eigen_invVarX$values) ^ 0.5 %*%
t(eigen_invVarX$vectors)
X.cs <- t(normalizing %*% (t(X) - colMeans(X)))
Y.CP.cs <- t(t(Y.CP) - colMeans(Y.CP))
library(MYHRcpp)
n <- 500
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(sin(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
SDR1 <- cumuSIR(X = X, Y = Y)
SDR2 <- cumuSAVE(X = X, Y = Y)
SDR1
SDR2
n <- 500
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(exp(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
SDR1 <- cumuSIR(X = X, Y = Y)
SDR2 <- cumuSAVE(X = X, Y = Y)
SDR1$basis[, 1]
SDR2$basis[, 1]
library(MYHRcpp)
n <- 500
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- as.matrix(exp(X %*% rep(1, p)) + rnorm(n, mean = 0, sd = 0.2))
SDR1 <- cumuSIR(X = X, Y = Y)
SDR2 <- cumuSAVE(X = X, Y = Y)
SDR1_wrong <- cumSIR_wrong(X = X, Y = Y)
SDR1_wrong <- cumuSIR_wrong(X = X, Y = Y)
SDR1_wrong$basis[, 1]
SDR1$basis[, 1]
library(MYHRcpp)
library(MYHRcpp)
##### Normal regression model #####
N_sim <- 10000
X1 <- rnorm(n = N_sim, mean = 0, sd = 1)
X2 <- rbinom(n = N_sim, size = 1, prob = 0.6)
X2
mean(X2)
sd(X2)
var(X2)
var(X2)^0.5
Y <- rnorm(n = N_sim, mean = 1 + X1 + X2, sd = 0.5)
hist(Y)
plot(Y, X1)
plot(Y, X2)
?rnorm
exp(6)
exp(-2)
exp(3)
##### Normal regression model #####
n <- 100
N <- 1000
N_sim <- 10000
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
Y_sim <- rnorm(n = N_sim, mean = 1 + X1_sim + X2_sim, sd = 0.5)
##### Normal regression model #####
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
Y <- rnorm(n = n, mean = 1 + X1 + X2, sd = 0.5)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
Y_sim <- rnorm(n = N_sim, mean = 1 + X1_sim + X2_sim, sd = 0.5)
##### Normal regression model #####
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
Y <- rnorm(n = n, mean = 1 + X1 + X2, sd = 0.5)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
Y_sim <- rnorm(n = N_sim, mean = 1 + X1_sim + X2_sim, sd = 0.5)
Y_shift <- sample(Y, size = N, replace = TRUE, prob = exp(0.5 * Y_sim))
##### Normal regression model #####
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
Y <- rnorm(n = n, mean = 1 + X1 + X2, sd = 0.5)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
Y_sim <- rnorm(n = N_sim, mean = 1 + X1_sim + X2_sim, sd = 0.5)
Y_shift <- sample(x = Y_sim, size = N, replace = TRUE, prob = exp(0.5 * Y_sim))
hist(Y_shift)
hist(Y_sim)
mean(Y_shift)
mean(Y_sim)
var(Y_sim)
var(Y_shift)
?sample
##### Normal regression model #####
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
Y <- rnorm(n = n, mean = 1 + X1 + X2, sd = 0.5)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
Y_sim <- rnorm(n = N_sim, mean = 1 + X1_sim + X2_sim, sd = 0.5)
Y_shift <- sample(x = Y_sim, size = N, replace = TRUE, prob = exp(0.7 * Y_sim))
hist(Y_sim)
hist(Y_shift)
summary(Y_sim)
summary(Y_shift)
##### Normal regression model #####
beta0 <- 1
beta1 <- 1
beta2 <- 1
sigma0 <- 0.5
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
X <- cbind(X1, X2)
Y <- rnorm(n = n, mean = beta0 + X1 * beta1 + X2 * beta2, sd = sigma0)
##### Normal regression model #####
theta0 <- 1
theta1 <- 1
theta2 <- 1
sigma0 <- 0.5
beta0 <- 0.8
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
X <- cbind(X1, X2)
Y <- rnorm(n = n, mean = theta0 + X1 * theta1 + X2 * theta2, sd = sigma0)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
X_sim <- cbind(X1_sim, X2_sim)
Y_sim <- rnorm(n = N_sim,
mean = theta0 + X1_sim * theta1 + X2_sim * theta2,
sd = sigma0)
Y_shift <- sample(x = Y_sim, size = N, replace = TRUE, prob = exp(beta0 * Y_sim))
X_shift <- matrix(0, nrow = N, ncol = 2)
?dnorm
i <- 1
w_i <- dnorm(Y_shift[i] - theta0 - X1_sim * theta1 - X2_sim * theta2,
mean = 0, sd = sigma0)
wi
w_i
sample(1:N_sim, size = 1, prob = w_i)
X_shift[i, ] <- X_sim[sample(1:N_sim, size = 1, prob = w_i), ]
##### Normal regression model #####
theta0 <- 1
theta1 <- 1
theta2 <- 1
sigma0 <- 0.5
beta0 <- 0.8
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
X <- cbind(X1, X2)
Y <- rnorm(n = n, mean = theta0 + X1 * theta1 + X2 * theta2, sd = sigma0)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
X_sim <- cbind(X1_sim, X2_sim)
Y_sim <- rnorm(n = N_sim,
mean = theta0 + X1_sim * theta1 + X2_sim * theta2,
sd = sigma0)
Y_shift <- sample(x = Y_sim, size = N, replace = TRUE, prob = exp(beta0 * Y_sim))
X_shift <- matrix(0, nrow = N, ncol = 2)
for (i in 1:N)
{
w_i <- dnorm(Y_shift[i] - theta0 - X1_sim * theta1 - X2_sim * theta2,
mean = 0, sd = sigma0)
X_shift[i, ] <- X_sim[sample(1:N_sim, size = 1, prob = w_i), ]
}
colMeans(X_shift)
colMeans(X_sim)
hist(X_shift[, 1])
hist(X_sim[, 1])
X1_shift <- X_shift[, 1]
X2_shift <- X_shift[, 2]
X2_shift
quantile(Y_sim, c(0.25, 0.5, 0.75))
X_sim[Y_sim < quantile(Y_sim, 0.25), ]
colMeans(X_sim[Y_sim < quantile(Y_sim, 0.25), ])
phi1_0 <- colMeans(X_sim[Y_sim < quantile(Y_sim, 0.25), ])
phi2_0 <- colMeans(X_sim[(Y_sim >= quantile(Y_sim, 0.25)) &
(Y_sim < quantile(Y_sim, 0.5)), ])
phi1_0
phi2_0
phi1_0 <- colMeans(X_sim[Y_sim < quantile(Y_sim, 0.25), ])
phi2_0 <- colMeans(X_sim[(Y_sim >= quantile(Y_sim, 0.25)) &
(Y_sim < quantile(Y_sim, 0.5)), ])
phi3_0 <- colMeans(X_sim[(Y_sim >= quantile(Y_sim, 0.5)) &
(Y_sim < quantile(Y_sim, 0.75)), ])
phi1_0
phi2_0
phi3_0
phi1_0 <- colMeans(X_sim[Y_sim <= quantile(Y_sim, 0.25), ])
phi2_0 <- colMeans(X_sim[(Y_sim > quantile(Y_sim, 0.25)) &
(Y_sim <= quantile(Y_sim, 0.5)), ])
phi3_0 <- colMeans(X_sim[(Y_sim > quantile(Y_sim, 0.5)) &
(Y_sim <= quantile(Y_sim, 0.75)), ])
phi3_0 <- colMeans(X_sim[Y_sim > quantile(Y_sim, 0.75), ])
phi1_0
phi2_0
phi3_0
phi4_0
phi1_0 <- colMeans(X_sim[Y_sim <= quantile(Y_sim, 0.25), ])
phi2_0 <- colMeans(X_sim[(Y_sim > quantile(Y_sim, 0.25)) &
(Y_sim <= quantile(Y_sim, 0.5)), ])
phi3_0 <- colMeans(X_sim[(Y_sim > quantile(Y_sim, 0.5)) &
(Y_sim <= quantile(Y_sim, 0.75)), ])
phi4_0 <- colMeans(X_sim[Y_sim > quantile(Y_sim, 0.75), ])
phi4_0
phi3_0
phi2_0
phi1_0
##### Normal regression model #####
theta0 <- 1
theta1 <- 1
theta2 <- 1
sigma0 <- 0.5
beta0 <- 0.8
n <- 100
N <- 1000
N_sim <- 10000
X1 <- rnorm(n = n, mean = 0, sd = 1)
X2 <- rbinom(n = n, size = 1, prob = 0.6)
X <- cbind(X1, X2)
Y <- rnorm(n = n, mean = theta0 + X1 * theta1 + X2 * theta2, sd = sigma0)
X1_sim <- rnorm(n = N_sim, mean = 0, sd = 1)
X2_sim <- rbinom(n = N_sim, size = 1, prob = 0.6)
X_sim <- cbind(X1_sim, X2_sim)
Y_sim <- rnorm(n = N_sim,
mean = theta0 + X1_sim * theta1 + X2_sim * theta2,
sd = sigma0)
Y_shift <- sample(x = Y_sim, size = N, replace = TRUE, prob = exp(beta0 * Y_sim))
X_shift <- matrix(0, nrow = N, ncol = 2)
for (i in 1:N)
{
w_i <- dnorm(Y_shift[i] - theta0 - X1_sim * theta1 - X2_sim * theta2,
mean = 0, sd = sigma0)
X_shift[i, ] <- X_sim[sample(1:N_sim, size = 1, prob = w_i), ]
}
X1_shift <- X_shift[, 1]
X2_shift <- X_shift[, 2]
### average of X given Y
phi1 <- colMeans(X_shift[Y_shift <= quantile(Y_shift, 0.25), ])
phi2 <- colMeans(X_shift[(Y_shift > quantile(Y_shift, 0.25)) &
(Y_shift <= quantile(Y_shift, 0.5)), ])
phi3 <- colMeans(X_shift[(Y_shift > quantile(Y_shift, 0.5)) &
(Y_shift <= quantile(Y_shift, 0.75)), ])
phi4 <- colMeans(X_shift[Y_shift > quantile(Y_shift, 0.75), ])
phi1
phi2
phi3
phi4
